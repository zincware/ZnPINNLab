{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6781e738",
   "metadata": {},
   "source": [
    "### Nomenclature \n",
    "NN      Neural Network  \n",
    "PINN       Physics-Informed Neural \n",
    "___\n",
    "\n",
    "# Background \n",
    "The goal of today's experiment is to train a neural network (NN) and a physics-informed neural network (PINN) to predict the trajectory of a projectile motion. We'll be exploring the influence of hyperparameters and the number of colocation points on the training, and comparing the NN with the PINN.\n",
    "Throughout the experiment, we'll be applying machine learning theory practically and familiarizing ourselves with the PyTorch library. The experiment focuses on a simple physical system to emphasize the machine learning aspects.\n",
    "\n",
    "# Workflow\n",
    "This Notebook is a tutorial designed for hands-on learning. Please follow the chronological order unless advised otherwise.  \n",
    "You will implement a neural network (NN) and a physics-informed neural network (PINN) and complete various execution tasks.  \n",
    "Good luck!\n",
    "___\n",
    "# Chronological order\n",
    "## NN implementation and execution\n",
    "### Implementation:\n",
    "First you will implement a NN. This includes the following steps:\n",
    "- importing and preparing data.\n",
    "- creating the NN architecture.\n",
    "- defining the data- and total loss functions.\n",
    "- writing the training procedure.\n",
    "\n",
    "\n",
    "### Execution tasks:\n",
    "After implementing the code, it is necessary to test its functionality. Proceed to the following execution tasks:\n",
    "1. __Verification__:  \n",
    "    Execute training and visualize the results.\n",
    "2. __Hyperparameter__:  \n",
    "    Test the impact of hyperparameters on model optimization.\n",
    "\n",
    "## PINN implementation and execution\n",
    "### Implementation:\n",
    "Implement a physics-informed neural network (PINN) using the steps outlined below:\n",
    "- Implement the function `compute_physics_informed_loss`\n",
    "- Include the Physics Loss in `compute_total_loss`\n",
    "\n",
    "### Execution tasks:\n",
    "The work is finalized with the following two execution tasks:\n",
    "\n",
    "3. __NN vs. PINN__:  \n",
    "    Compare the performance of PINN and regular NN on different datasets.  \n",
    "4. __Collocation points__:  \n",
    "    Identify the effect of the collocation points when training a PINN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50243fc8",
   "metadata": {},
   "source": [
    "# Implementation NN Part 1\n",
    "Import all necessary libraries and classes by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4b3dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "from utils.physics_data_generator import ProjectileDataGenerator, PhysicalSystem\n",
    "from utils.Animator import AnimationGenerator, SolutionVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9a384f",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "\n",
    "The data we are working with describes the trajectory of a projectile that is slowed down by a drag force.\n",
    "It is generated using two imported classes.  \n",
    "- The `physical_system` class contains all relevant information about the projectile and its motion.  \n",
    "- To create labeled data, we use the `data_generator`, which produces time values and corresponding position targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fdcaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use existing code to generate training data\n",
    "physical_system = PhysicalSystem()\n",
    "data_generator = ProjectileDataGenerator(physical_system)\n",
    "\n",
    "# Generate training data\n",
    "data_generator.integrate()\n",
    "\n",
    "# Get training data\n",
    "time = data_generator.time\n",
    "position = data_generator.position\n",
    "\n",
    "# Get the physical system parameters\n",
    "g = physical_system.g\n",
    "Cd = physical_system.drag_coeff\n",
    "A = physical_system.cross_area\n",
    "m = physical_system.mass\n",
    "mu = physical_system.coeff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8455dabd",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "\n",
    "To better understand the data, visualize the projectile's trajectory. Half of the necessary code has already been provided.  \n",
    "To create the plot, replace the '...' with the correct code.  \n",
    "The plot should include the following properties:\n",
    "- Plot the x-position on the x-axis using the first column of the position tensor.\n",
    "- Plot the y-position on the y-axis using the second column of the position tensor.\n",
    "\n",
    "__Hint:__  \n",
    "First, determine the shape of the position tensor, and then use slicing to access the appropriate dimensions.  \n",
    "Learn how to select coloums via slicing [here](https://note.nkmk.me/en/python-numpy-ndarray-slice/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f9ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "#plt.plot(...)\n",
    "plt.title(rf\"Projectile Motion with Drag $\\mu$ = {physical_system.drag_coeff:.2f}\")\n",
    "plt.xlabel(\"x [m]\")\n",
    "plt.ylabel(\"y [m]\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f0779c",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "This section covers data preparation for training.  \n",
    "Data can be used for training and testing. To obtain accurate testing results, it is crucial to avoid using the same data for testing as for training. The dataset is therefore split into two complementary sets.\n",
    "\n",
    "Therefore, the `prepare_data` function indexes each value of the data set. Then, it selects an evenly distributed amount of 'num_train' indices for training purposes and uses the rest for testing:\n",
    "\n",
    "| `idx_train` | `all_idx` | `idx_test` |\n",
    "|:---------:|:---------:|:---------:|\n",
    "| 1 | &larr; 1 |  |\n",
    "|  | 2 &rarr; | 2 |\n",
    "|  | 3 &rarr; | 3 |\n",
    "| 4 | &larr; 4 |  |\n",
    "|  | 5 &rarr; | 5 |\n",
    "| ... | ... | ... |\n",
    "\n",
    "(This is just an example.)\n",
    "\n",
    "The data is then saved in dictionaries `train_ds` and `test_ds`. `time_vals` are used as inputs and `position_vals` as targets.\n",
    "\n",
    "Implement this logic by completing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d94354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(\n",
    "        time_vals : torch.Tensor, \n",
    "        position_vals : torch.Tensor, \n",
    "        num_train : int = 20,  \n",
    "        max_idx : int = len(time)-1,\n",
    "        num_t_col : int = 100,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create training data for training procedure.\n",
    "\n",
    "    This function generates training data using a subset\n",
    "    of time values ('time_vals' tensor) and corresponding \n",
    "    x- and y-components ('position_vals' tensor).\n",
    "    Therefore the following steps need to be done:\n",
    "    \n",
    "\n",
    "    Step 1: Data Point Selection\n",
    "            To split the data into training and testing data, \n",
    "            three Index tensors need to be created.\n",
    "\n",
    "            The 'all_idx' tensor includes indices from 0 to the dataset's length.\n",
    "            The 'idx_train' tensor includes 'num_train' evenly spaced indices for training.\n",
    "            The 'idx_test' tensor includes the remaining indices not in 'idx_train'.\n",
    "\n",
    "            - Create a tensor 'all_idx' containing indices between 0 \n",
    "              and 'len(time_vals)' to select data points.\n",
    "            - Create a tensor 'idx_train' containing 'num_train'evenly spaced indices \n",
    "              between 0 and max_idx to select data points.\n",
    "            - Generate a mask with booleans of len(all_idx) that gives False statements \n",
    "              for entries that are both in 'all_idx' and in 'idx_train'. (Just inspect the code)\n",
    "              (see [torch.isin](https://pytorch.org/docs/stable/generated/torch.isin.html)))\n",
    "            - Build a tensor 'idx_test' by applying the mask on 'all_idx'. (Just inspect the code)\n",
    "            - Ensure that the data type of all idx tensors are integer.\n",
    "\n",
    "    Step 2: Training Data Creation\n",
    "            - Define 't_train', 'x_train', and 'y_train' tensors \n",
    "              by selecting index values 'idx_train' from 'time_vals' and 'position_vals'.\n",
    "            - We don't want to make changes to the original data \n",
    "              therefore use '.clone().detach()'.\n",
    "            - The neural network expects a tensors with each value in a separate row.\n",
    "              Format the tensor size as 'torch.Size([num_train, 1])'\n",
    "              by applying the .unsqueeze(1) method.\n",
    "        \n",
    "    Step 3: Testing Data Creation\n",
    "            Repeat the procedure of step to by selecting index values 'idx_test'.\n",
    "    \n",
    "    Step 4: Colocation points\n",
    "            For the physics loss, we need evenly spaced time values \n",
    "            between 0 and the last time value.\n",
    "            - Using the torch.linspace() method, build 't_col', \n",
    "              a tensor that includes 'num_t_col' evenly spaced values \n",
    "              between 0 and the last value in 'time_vals'.\n",
    "              (see [torch.linspace](https://pytorch.org/docs/stable/generated/torch.linspace.html))\n",
    "            - Use the .unsqueeze(1) method.\n",
    "            - To declare the tensor as a trainable parameter,\n",
    "              Ensure that the tensor requires gradient.\n",
    "              (see [torch.Tensor.requires_grad](https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html)\n",
    "\n",
    "    Step 5: Dataset Creation\n",
    "            Create datasets train_ds and test_ds by using dictionaries \n",
    "            containing \"inputs\", \"targets_x\", \"targets_y\" and \"t_col\" entries.\n",
    "        \n",
    "    Step 6:  Return the created datasets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    time_vals : torch.Tensor\n",
    "        A tensor containing time values.\n",
    "    position_vals : torch.Tensor\n",
    "        A tensor containing x- and y-components of the displacement vector.\n",
    "    num_train : int, optional\n",
    "        The number of data points to select from the provided labeled dataset. Default is 20.\n",
    "    max_idx : float, optional\n",
    "        The maximum index value to select from data. Default is len(time)-1.\n",
    "    num_t_col : int, optional\n",
    "        The number of evenly spaced time values used as colocation points for the physics loss.\n",
    "        Default is 100.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict\n",
    "        A dictionary containing \"inputs\", \"targets_x\", \"targets_y\", \n",
    "        and \"t_col\" entries for training data.\n",
    "    Dict\n",
    "        A dictionary containing \"inputs\", \"targets_x\", \"targets_y\", \n",
    "        and \"t_col\" entries for testing data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Create a tensors of indices that correspond to the selected time values.\n",
    "    #all_idx = \n",
    "    #idx_train = \n",
    "    mask = ~torch.isin(all_idx, idx_train)\n",
    "    idx_test = all_idx[mask]\n",
    "\n",
    "    # Assertion checks: Does assertion statements to ensure the correctness of your implementation.\n",
    "    assert len(idx_train) + len(idx_test) == len(all_idx), \"The sum of training and testing indices should be equal to the total number of indices.\"\n",
    "    assert len(idx_train) == num_train, \"The number of training indices should be equal to the specified number of training data points.\"\n",
    "    assert idx_train[-1] == max_idx, \"The last index of the training indices should be equal to the maximum index value.\"\n",
    "    assert all_idx.dtype == torch.int, \"The data type of the indices should be integer.\"\n",
    "    assert idx_train.dtype == torch.int, \"The data type of the training indices should be integer.\"\n",
    "    \n",
    "    # Step 2: Get 't_train', 'x_train' and 'y_train' by selecting values from the provided data.\n",
    "    #t_train = time_vals[idx_train].clone().detach().unsqueeze(1)\n",
    "    #x_train = \n",
    "    #y_train = \n",
    "\n",
    "    # Assertion check\n",
    "    assert t_train.shape == x_train.shape == y_train.shape == torch.Size([num_train, 1]), \"The shapes of t_train, x_train, and y_train should be torch.Size([num_train, 1]).\"\n",
    "    \n",
    "    \n",
    "    # Step 3: Get 't_test', 'x_test', 'y_test' from the provided data\n",
    "    #t_test = \n",
    "    #x_test = \n",
    "    #y_test = \n",
    "\n",
    "    # Assertion check\n",
    "    assert t_test.shape == x_test.shape == y_test.shape == torch.Size([len(time_vals)-num_train, 1]), \"The shapes of t_test, x_test, and y_test should be torch.Size([len(time_vals)-num_train, 1]).\"\n",
    "\n",
    "\n",
    "    # Step 4: Generate a t_col tensor.\n",
    "    # t_col = \n",
    "\n",
    "    # Assertion check\n",
    "    assert t_col.shape == torch.Size([num_t_col, 1]), \"The shape of t_col should be torch.Size([num_t_col, 1]).\"\n",
    "        \n",
    "    \n",
    "    # Step 5: Create datasets 'train_ds' and 'test_ds' by using dictionaries \n",
    "    #train_ds = \n",
    "    #test_ds = \n",
    "\n",
    "    # Assertion check\n",
    "    assert len(train_ds) == len(test_ds) == 4, \"The datasets should contain four entries.\" \n",
    "\n",
    "    # Step 6: return datasets\n",
    "    pass # replace the pass statement with the return statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f15bb7",
   "metadata": {},
   "source": [
    "__Checkpoint__:  \n",
    "In the following cell, run the function. If you have implemented everything correctly, the assert statements should not produce any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c150c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = prepare_data(time, position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ad1918",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be1d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    \"\"\"\n",
    "    Physics-Informed Neural Network (PINN) Class\n",
    "    \n",
    "    This class as a subclass of nn.Module defines the architecture of the PINN model. \n",
    "    It is designed to use differential equations while incorporating physics-based constraints.\n",
    "    The process consists of the following steps:\n",
    "    \n",
    "    Step 1: Model Initialization (Just inspect the code)\n",
    "        - Initialize the PINN model as a subclass of nn.Module.\n",
    "\n",
    "    Step 2: Constructor Definition (Just inspect the code)\n",
    "        - Build a constructor to configure the model's architecture.\n",
    "        - Utilize the nn.Linear class from the PyTorch library \n",
    "          for defining layers and connections. \n",
    "\n",
    "    Step 3: Forward Pass Mechanism\n",
    "        - Define the forward pass mechanism for the model, \n",
    "          where input data flows through the layers\n",
    "          to produce predicted outputs.\n",
    "       \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor for the PINN class.\n",
    "        \n",
    "        Initializes with the xavier_uniform function the layers of the neural network:\n",
    "        (see [Xavier_uniform](https://pytorch.org/cppdocs/api/function_namespacetorch_1_1nn_1_1init_1ace282f75916a862c9678343dfd4d5ffe.html))])\n",
    "        (see [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html))\n",
    "        - Input layer fc1 taking a tensor with time data.\n",
    "        - One hidden fully connected layer fc2 with 64 neurons.\n",
    "        - Output layer fc_x for predicting the x-coordinate.\n",
    "        - Output layer fc_y for predicting the y-coordinate.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "            \n",
    "        Attributes\n",
    "        ----------\n",
    "        fc1 : nn.Linear\n",
    "            First fully connected layer.\n",
    "        fc2 : nn.Linear\n",
    "            Second fully connected layer.\n",
    "        fc_x : nn.Linear\n",
    "            Output layer for x-coordinate prediction.\n",
    "        fc_y : nn.Linear\n",
    "            Output layer for y-coordinate prediction.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "        super(PINN, self).__init__()\n",
    "        \n",
    "        # Step 2: Configure model architecture as described earlier\n",
    "        torch.manual_seed(42)  # Set seed for reproducibility\n",
    "        self.fc1 = nn.Linear(1, 64)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        self.fc_x = nn.Linear(64, 1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc_x.weight)\n",
    "        self.fc_y = nn.Linear(64, 1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc_y.weight)\n",
    "    \n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the PINN model.\n",
    "        \n",
    "        This method defines the forward pass mechanism of the PINN model, where\n",
    "        the input data t is processed through the layers to produce predicted\n",
    "        outputs for both x-coordinate (x_output) and y-coordinate (y_output).\n",
    "        The following steps are necessary:\n",
    "        \n",
    "    1. First Fully Connected Layer with GELU Activation:\n",
    "        - Pass the input tensor 't' through the 'fc1' linear layer.\n",
    "        - Apply the GELU activation function 'torch.nn.functional.gelu(...)' to the output.\n",
    "\n",
    "    2. Second Fully Connected Layer with GELU Activation:\n",
    "        - Pass the output of the previous step ('t') through the 'fc2' linear layer.\n",
    "        - Apply the GELU activation function to the output.\n",
    "\n",
    "    3. Output Layer for Predicted Coordinates:\n",
    "        - Compute the predicted x-coordinate by passing the transformed tensor 't' \n",
    "          through the 'fc_x' layer.\n",
    "        - Compute the predicted y-coordinate by passing the same transformed tensor 't' \n",
    "          through the 'fc_y' layer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        t : torch.Tensor\n",
    "            Input data tensor.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        x_output : torch.Tensor\n",
    "            Predicted x-coordinate.\n",
    "        y_output : torch.Tensor\n",
    "            Predicted y-coordinate.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Apply the first fully connected layer with GELU activation\n",
    "        t = torch.nn.functional.gelu(self.fc1(t))\n",
    "        \n",
    "        # Step 2: Apply the second fully connected layer with GELU activation\n",
    "        #t =\n",
    "        \n",
    "        # Step 3: Produce predicted x and y coordinates using output layers and return them.\n",
    "        #x_output =   # Predicted x-coordinate\n",
    "        #y_output =   # Predicted y-coordinate\n",
    "        \n",
    "        pass #replace the pass with the return of the predicted x and y coordinates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fbf857",
   "metadata": {},
   "source": [
    "## Data loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d437547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_data_loss(model : nn.Module, dataset: dict):\n",
    "    \"\"\"\n",
    "    Define the data loss for the PINN model.\n",
    "    \n",
    "    This function calculates the loss based on the discrepancy between the predicted\n",
    "    and actual data points, typically used for data-driven training of the PINN. The\n",
    "    following steps are involved:\n",
    "    \n",
    "\n",
    "    Step 1: Model Prediction\n",
    "        - Compute the predicted x and y values by calling the neural network model.\n",
    "          on the \"inputs\" entry of the dataset.\n",
    "\n",
    "    Step 2: Loss Calculation\n",
    "        - Calculate the mean squared error loss for both the x and y components using\n",
    "        the 'nn.MSELoss' class.\n",
    "        (see [MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html))\n",
    "\n",
    "    Step 3: Loss Combination\n",
    "        - Return the combined data loss as the sum of 'loss_x' and 'loss_y'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The physics-informed neural network model.\n",
    "    dataset : dict\n",
    "        A dictionary containing \"inputs\", \"targets_x\", \"targets_y\" and \"t_col\" entries.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        The combined data loss based on predicted vs. actual data.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute predicted x and y values using the neural network model.\n",
    "    #         Replace ... with the appropriate code.\n",
    "    #x_pred, y_pred = model(...)\n",
    "    \n",
    "    # Step 2: Calculate mean squared error loss\n",
    "    mse_loss = nn.MSELoss()\n",
    "    #loss_x = mse_loss(x_pred, dataset[\"targets_x\"])\n",
    "    #loss_y = \n",
    "\n",
    "    # Assert section: Ensures the correct shapes and values (explanation in the next cell)\n",
    "    if model == Dummy_NN:\n",
    "        assert x_pred.shape == (20, 1), \"Incorrect shape for x_pred\"\n",
    "        assert y_pred.shape == (20, 1), \"Incorrect shape for y_pred\"\n",
    "        assert round(x_pred[0,0].item(), 6) == -0.066156, \"Incorrect value for x_pred\"\n",
    "        assert round(y_pred[0,0].item(), 6) == -0.310818, \"Incorrect value for y_pred\"\n",
    "        assert round(loss_x.item(), 6) == 560.922668, \"Incorrect value for loss_x\"\n",
    "        assert round(loss_y.item(), 6) == 209.860199, \"Incorrect value for loss_y\"\n",
    "    \n",
    "    # Step 3: Return the combined (x and y) data loss\n",
    "    pass # replace the pass statement with the return statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c8a320",
   "metadata": {},
   "source": [
    "__Checkpoint:__  \n",
    "In the next cell, you will test your defined ``compute_data_loss`` function on a generated dataset ``dummy_train_ds`` and an instance ``Dummy_NN`` of the PINN.  \n",
    "If implemented correctly, the prediction/ and loss values/ and shapes pruduced inside the ``compute_data_loss`` function will always be the same, regardless of the randomly generated network parameters.\n",
    "This is due to the predefined seed value used for parameter initialization, which ensures consistent distribution every time.  \n",
    "\n",
    "If you run the following cell and no error occurs, it means that your `compute_data_loss` and the `PINN` class have been implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73817f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dummy_NN = PINN()\n",
    "\n",
    "dummy_train_ds, dummy_test_ds = prepare_data(time, position)\n",
    "\n",
    "dummy_data_loss = compute_data_loss(Dummy_NN, dummy_train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aefdf16",
   "metadata": {},
   "source": [
    "# Implementation PINN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01042b9",
   "metadata": {},
   "source": [
    "\n",
    "## Physicis informed loss\n",
    "\n",
    "If you come here for the first time and you have just implemented the `compute_data_loss` function,  \n",
    "leave the the `compute_physics_informed_loss` function as it is. \n",
    "\n",
    "If you come here for the second time, complete the `compute_physics_informed_loss` function implementation.  \n",
    "You will need some physics background information:\n",
    "___\n",
    "\n",
    "### Physics background\n",
    "\n",
    "This PINN aims to learn and predict the trajectory of a projectile slowed down by a drag force.\n",
    "\n",
    "The magnitude of the force depends on the following parameters:\n",
    "1. $\\rho$ the Air density $(\\text{kg}/\\text{m}^3)$, \n",
    "2. $C_d$ the drag coefficient, \n",
    "3. $A$ the Cross-sectional area $(\\text{m}^2)$\n",
    "\n",
    "All these parameters can be summarized to one coefficient:\n",
    "- $\\mu = 0.5 * \\rho * C_d * A$\n",
    "\n",
    "Moreover, the drag force depends on the square of the projectile's speed:\n",
    "- $\\vec F = - \\mu * \\vec v * |\\vec v|$ \n",
    "where $\\vec F$ can be split up in $x$-and $y$-components:\n",
    "- $F_x = - \\mu * \\frac{dx}{dt} * |\\vec v|$ and \n",
    "- $F_y = - \\mu * \\frac{dy}{dt} * |\\vec v|$\n",
    "\n",
    "The resulting differential equations describe the path of the projectile:\n",
    "- $\\frac{d^2x}{dt^2} = F_x/m$\n",
    "- $\\frac{d^2y}{dt^2} = F_y/m - g$ \n",
    "\n",
    "Where $m$ is the Mass of the projectile $(\\text{kg})$ and $g$ the earths gravitational acceleration. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac49c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_physics_informed_loss(model : nn.Module, dataset: dict):\n",
    "    \"\"\"\n",
    "    Define the physics-informed loss for the PINN model.\n",
    "    \n",
    "    This function calculates the loss used to incorporate the underlying physics\n",
    "    principles into the PINN. The following steps are involved:\n",
    "    \n",
    "    Step 1: Data Preparation\n",
    "            - Unpack 't_col' from the dataset.\n",
    "\n",
    "    Step 2: Model Prediction\n",
    "            - Predict x and y values by calling the neural network model on 't_col'.\n",
    "\n",
    "    Step 3: Gradient Computation\n",
    "            - Compute the first gradients 'dx_dt' and 'dy_dt', and second gradients \n",
    "            'd2x_dt2' and 'd2y_dt2', using the 'torch.autograd.grad' method.\n",
    "            (see[grad](https://pytorch.org/docs/stable/generated/torch.autograd.grad.html))\n",
    "\n",
    "    Step 4: Velocity and Drag Calculation\n",
    "            - Calculate the speed 'v' using the Euclidean norm of the vector [dx_dt, dy_dt].\n",
    "            - Define the x and y components of the drag force.\n",
    "\n",
    "    Step 5: Loss Calculation\n",
    "            - Calculate the mean squared error loss for both the x and y components using\n",
    "            the 'nn.MSELoss' class.\n",
    "            (see [MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html))\n",
    "\n",
    "    Step 6: Loss Combination\n",
    "            - Return the combined physics-informed loss as the sum of 'loss_x' and 'loss_y'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The physics-informed neural network model.\n",
    "    dataset : dict\n",
    "        A dictionary containing \"inputs\", \"targets_x\", \"targets_y\" and \"t_col\" entries.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        The combined (x and y) physics-informed loss.\n",
    "    \"\"\"\n",
    "    # Step 1: Unpack t_col from the dataset\n",
    "    #t_col = \n",
    "    \n",
    "    # Step 2: Predict x and y values using the neural network model\n",
    "    #x_pred, y_pred = \n",
    "    \n",
    "    # Step 3: Compute first and second gradients\n",
    "    #dx_dt = torch.autograd.grad(x_pred, t_col, grad_outputs=torch.ones_like(x_pred), create_graph=True)[0]\n",
    "    #d2x_dt2 = \n",
    "    \n",
    "    #dy_dt = \n",
    "    #d2y_dt2 = \n",
    "\n",
    "    # Assertion checks\n",
    "    if model == Dummy_NN:\n",
    "        assert round(dy_dt[0,0].item(), 6) == 0.010456 , \"Incorrect value for dy_dt, check the gradient computation\"\n",
    "        assert round(d2y_dt2[0,0].item(), 6) == -0.011869 , \"Incorrect value for d2y_dt2, check the gradient computation\"\n",
    "        assert round(d2x_dt2[0,0].item(), 6) == -0.025199 , \"Incorrect value for d2x_dt2, check the gradient computation\"\n",
    "    \n",
    "    # Step 4: Calculate the speed v using the square root (torch.sqrt()) of the sum \n",
    "    #         of squares of dx_dt and dy_dt and define x and y component of the drag force.\n",
    "    #v = \n",
    "    #Fx = -mu \n",
    "    #Fy = \n",
    "    \n",
    "    # Step 5: Compute the mean squared error loss \n",
    "    #         and store by defining variables called 'loss_x' and 'loss_y'.\n",
    "    #mse_loss = \n",
    "    #loss_x = \n",
    "    #loss_y = \n",
    "\n",
    "    # Step 6: Return the combined (sum of x and y) physics-informed loss\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7554d356",
   "metadata": {},
   "source": [
    "__Checkpoint:__  \n",
    "If you run the following cell and get a value of __96.1587__, it means that your `compute_physics_informed_loss` function has been implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ab73f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_physics_informed_loss = compute_physics_informed_loss(Dummy_NN, dummy_train_ds)\n",
    "\n",
    "print(\"Dummy physics informed Loss:\",dummy_physics_informed_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3c9aac",
   "metadata": {},
   "source": [
    "# Implementation NN Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e97d67e",
   "metadata": {},
   "source": [
    "## Total Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce5814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_loss(model : nn.Module, dataset: dict, activate_physics=False):\n",
    "    \"\"\"\n",
    "    Define the total loss for the physics-informed neural network.\n",
    "\n",
    "    This function computes the total loss for the PINN model by combining two\n",
    "    different components: data loss and physics-informed loss.\n",
    "    The following steps are involved:\n",
    "\n",
    "    Step 1: Data Loss Calculation\n",
    "        - Determine the data loss using the 'compute_data_loss' function.\n",
    "\n",
    "    Step 2: Physics-Informed Loss Calculation (if already implemented). If not proceed to Step 3)\n",
    "        - If 'activate_physics' is set to True, calculate the physics-informed loss\n",
    "        using the 'compute_physics_informed_loss' function.\n",
    "        - Else, set the physics loss to zero.\n",
    "\n",
    "    Step 3: Total Loss Combination\n",
    "        - Return the combined total loss \n",
    "          as the sum of 'data_loss' and 'physics_loss' (if already implemented).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The physics-informed neural network model.\n",
    "    dataset : dict\n",
    "        A dictionary containing \"inputs\", \"targets_x\", \"targets_y\", and \"t_col\" entries.\n",
    "    activate_physics : bool, optional\n",
    "        Whether to activate the physics-informed loss. Default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        The combined total loss considering data and physics constraints.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate the data loss\n",
    "    #data_loss = \n",
    "\n",
    "    # Step 2: Calculate the physics-informed loss if 'activate_physics' is True\n",
    "\n",
    "\n",
    "    # Step 3: Combine data loss and physics loss (if activated) and return the total loss\n",
    "    \n",
    "    pass # replace the pass statement with the return statement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0636d67",
   "metadata": {},
   "source": [
    "__Checkpoint:__  \n",
    "If you run the following cell and no error occurs, it means that your `compute_total_loss` function has been implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c04d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_total_loss = compute_total_loss(Dummy_NN, dummy_train_ds)\n",
    "\n",
    "print(\"Dummy total Loss:\",dummy_total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84628961",
   "metadata": {},
   "source": [
    "## Executive function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0fb907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(\n",
    "        model : nn.Module,\n",
    "        train_ds: dict,\n",
    "        test_ds: dict,\n",
    "        lr : float = 0.01, \n",
    "        num_epochs : int = 300,\n",
    "        activate_physics : bool = False,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Execute the training procedure for a physics-informed neural network model.\n",
    "\n",
    "    This function trains the model using specified hyperparameters and returns relevant data.\n",
    "    The process involves the following steps:\n",
    "    \n",
    "    Step 1: Optimizer Initialization\n",
    "            The optimizer is used to update the model's parameters during training. Therefore:\n",
    "            - Define 'params' by calling the parameters() method on the model.\n",
    "              (see [PARAMETERS](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters))\n",
    "            - Configure the Adam optimizer with a specified learning rate 'lr' and the 'params'.\n",
    "              More informations on the Adam optimizer can be found here:\n",
    "              (see [ADAM](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html))\n",
    "\n",
    "\n",
    "    Inside the training loop:\n",
    "\n",
    "        Step 2: Loss Calculation\n",
    "                Calculate the 'train_loss' and 'test_loss' \n",
    "                using the 'compute_total_loss' function.\n",
    "                Make sure, 'activate_physics' is set to 'activate_physics'.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The neural network model to be trained and evaluated.\n",
    "    train_ds : dict\n",
    "        A dictionary containing training data, including inputs and targets.\n",
    "    test_ds : dict\n",
    "        A dictionary containing test data, including inputs and targets.\n",
    "    lr : float, optional\n",
    "        Learning rate for the optimizer. Default is 0.01.\n",
    "    num_epochs : int, optional\n",
    "        Number of training epochs. Default is 300.\n",
    "    activate_physics : bool, optional\n",
    "        Whether to activate the physics-informed loss. Default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict\n",
    "        A dictionary containing the following entries:\n",
    "        - \"train_loss_evolution\": A list of training loss values.\n",
    "        - \"test_loss_evolution\": A list of test loss values.\n",
    "        - \"predictions_list\": A list of model predictions.\n",
    "        - \"train_targets_x\": A tensor of x-coordinates of the training targets.\n",
    "        - \"train_targets_y\": A tensor of y-coordinates of the training targets.\n",
    "        - \"test_targets_x\": A tensor of x-coordinates of the test targets.\n",
    "        - \"test_targets_y\": A tensor of y-coordinates of the test targets.\n",
    "        if activate_physics:\n",
    "        - \"x_pred_col\": A tensor of x-coordinates of the colocation points.\n",
    "        - \"y_pred_col\": A tensor of y-coordinates of the colocation points.\n",
    "\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Step 1: Configure optimizer \n",
    "    # params = \n",
    "    optimizer = optim.Adam(...)  # Replace the '...' with the model parameters and the learning rate.\n",
    "    \n",
    "    # Initialize the dictionary and lists\n",
    "    predictions_dict = {}\n",
    "    train_loss_evolution = []\n",
    "    test_loss_evolution = []\n",
    "    predictions_list = []\n",
    "    \n",
    "    # Define Loading Bar\n",
    "    loading_bar = trange(1, num_epochs + 1)\n",
    "    \n",
    "    # Set up the training loop for the specified number of epochs ('num_epochs')\n",
    "    for epoch in loading_bar:\n",
    "\n",
    "        # Step 3: Compute the total loss for the training and the test data.\n",
    "        #train_loss = \n",
    "        #test_loss = \n",
    "        \n",
    "        # Calculate the training loss ('train_loss') gradients using backpropagation, \n",
    "        # then update the model parameters \n",
    "        # and set the gradients to zero.\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Append the current loss value in the loss lists\n",
    "        train_loss_evolution.append(float(train_loss))\n",
    "        test_loss_evolution.append(float(test_loss))\n",
    "           \n",
    "        # Compute the model prediction and store it in the respective list.\n",
    "        predictions = model(test_ds['inputs'])\n",
    "        predictions_list.append(predictions)\n",
    "\n",
    "        # Print current epoch and losses\n",
    "        loading_bar.set_description(f\"Epoch: {epoch}\")\n",
    "        loading_bar.set_postfix({\"Test Loss\": test_loss.item(), \"Train Loss\": train_loss.item()})\n",
    "\n",
    "    # Fill the dictionary as specified and return it.\n",
    "    predictions_dict[\"train_loss_evolution\"] = train_loss_evolution\n",
    "    predictions_dict[\"test_loss_evolution\"] = test_loss_evolution\n",
    "    predictions_dict[\"predictions_list\"] = predictions_list\n",
    "    predictions_dict[\"train_targets_x\"] = train_ds[\"targets_x\"]\n",
    "    predictions_dict[\"train_targets_y\"] = train_ds[\"targets_y\"]\n",
    "    predictions_dict[\"test_targets_x\"] = test_ds[\"targets_x\"]\n",
    "    predictions_dict[\"test_targets_y\"] = test_ds[\"targets_y\"]\n",
    "\n",
    "    if activate_physics:\n",
    "        x_pred_col, y_pred_col = model(test_ds[\"t_col\"])\n",
    "        predictions_dict[\"x_pred_col\"] = x_pred_col\n",
    "        predictions_dict[\"y_pred_col\"] = y_pred_col\n",
    "\n",
    "\n",
    "    return predictions_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c890de18",
   "metadata": {},
   "source": [
    "# Execution NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36b1af7",
   "metadata": {},
   "source": [
    "## Task 1: Verification\n",
    "Execute Training and Visualize \n",
    "    Execute the training for the following set of (hyper)parameters.  \n",
    "    Plot the loss evolution during training and the final prediction of the model (after training).\n",
    "    Compare the final predictions to the true solution of the trajectory. \n",
    "___\n",
    "### Datapreparation\n",
    "- Use default parameters for the data generation\n",
    "- Use __20 points__ (linearly spaced in time) from the generated data for the `train_ds`.  \n",
    "\n",
    "### Training execution\n",
    "- Learning rate $\\eta = 0.005$\n",
    "- Training epochs $n_\\text{epochs} = 300$\n",
    "- Set ``physics_activated`` = `False`\n",
    "___\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f3605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize the model by calling the PINN class created earlier.\n",
    "\n",
    "# Step 2: Generate training and test datasets by calling the 'prepare_data' function.\n",
    "\n",
    "# Step 3: Execute the training and testing of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7a54b3",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "In the upcoming execution tasks, you will need to visualize various model predictions.  \n",
    "To facilitate this process, you will use the SolutionVisualizer class.  \n",
    "### Task\n",
    "To understand how this class works, add the appropriate  \n",
    "code inside the `_customize_first_subplot` and `_customize_second_subplot` methods where indicated by (...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cefa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizer = SolutionVisualizer(predictions_dict)\n",
    "Visualizer.plot_solution(title=rf'Projectile Motion with Drag $\\mu$ = {physical_system.drag_coeff:.2f}') # Add title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349acd21",
   "metadata": {},
   "source": [
    "# Animation\n",
    "Congratulations! You have successfully trained the model. Now it's time to relax and watch a movie.  \n",
    "Please run the following cell to see the model training in action.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5fa541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "animator = AnimationGenerator(predictions_dict,\n",
    "                              train_ds, \n",
    "                              test_ds)\n",
    "animator.create_animation('lets_see.mp4', frames=len(predictions_dict[\"train_loss_evolution\"]), fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c38b893",
   "metadata": {},
   "source": [
    "## Task 2: Hyperparameters \n",
    "Finding the best configuration for a given task by tuning all hyperparameters of  \n",
    "the model, such as learning rate, model depth, and model width, can be a tedious process.  \n",
    "We already did some of that work for you, but you will now have the opportunity to test the impact of the learning rate.\n",
    "\n",
    "To roughly assess that, adjust the learning rate and observe the model's performance.\n",
    "1. Create the necessary datasets and implement a loop for the following list of learning rates.\n",
    "\n",
    "    Learning rates = [1, 1e-1, 1e-2, 1e-3, 1e-4]\n",
    "\n",
    "   Train the neural network in each loop using the configuration from task 1, except for the learning rate.  \n",
    "   Append the `prediction_dict`'s in a list called `pred_list`.\n",
    "\n",
    "   __Hint__:  \n",
    "   Don't forget to initialize a new ``model`` in each loop.\n",
    "\n",
    "2. Utilize the `SolutionVisualizer` class to compare the loss evolutions and final predictions in two plots.\n",
    "This will be done automatically by executing the cell below.\n",
    "3. Which learning rate yields the best results?  \n",
    "Explain why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab0264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dict that can store the prediction and loss data\n",
    "pred_list = []\n",
    "\n",
    "# Define the Looplist 'Learning_rates'\n",
    "Learning_rates = [1, 1e-1, 1e-2, 1e-3, 1e-4]\n",
    "\n",
    "# Create Training and Test datasets\n",
    "...\n",
    "\n",
    "# Loop through the Learning_rates and train the model \n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f07f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sub_dict in enumerate(pred_list):\n",
    "    Visualizer = SolutionVisualizer(sub_dict)\n",
    "    Visualizer.plot_solution(title=rf'Learning rate = {Learning_rates[i]}',\n",
    "                             filename=f\"lr_data_{i}.svg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e0a9c",
   "metadata": {},
   "source": [
    "# PINN\n",
    "Congratulations on completing all tasks related to the NN.  \n",
    "To proceed with the next Execution Task (cell below), you must first unlock the PINN functionalities.\n",
    "- Implement the `compute_physics_informed_loss` function [higher up in the code.](#physics-background) (You may need to scroll)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b899aef",
   "metadata": {},
   "source": [
    "# Execution PINN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd945b2d",
   "metadata": {},
   "source": [
    "## Task 3: NN vs. PINN\n",
    "The purpose of this task is to compare the performance of a regular NN and a PINN.  \n",
    "The comparison will be made by training both networks on labeled datasets with the following configurations:\n",
    "\n",
    "- 20 equally spaced training points over the entire data set.\n",
    "- 10 equally spaced training points over the first half of the data. \n",
    "- 2 training points using the first and last point of the data set.  \n",
    "\n",
    "1. For each of these data sets, execute one training of a PINN (with physics loss) and one training of regular NN (without physics loss) using the parameter configuration from task 1 (except the training data points). \n",
    "Moreover, we use the default values of:\n",
    "    - Learning rate $\\eta = 0.005$\n",
    "    - Training epochs $n_\\text{epochs} = 500$\n",
    "\n",
    "\n",
    "**Your task** is to write the executive code to run and save the training for each configuration using the following steps:\n",
    "- Use the `prepare_data` function to create the datasets.\n",
    "- Train the PINN/NN using the `train_model` function.\n",
    "- Save the `prediction_dict`s in a list called `pred_list`.\n",
    "The location in the code where you should implement the steps is indicated by `(...)`.\n",
    "\n",
    "2. Compare the results using the `SolutionVisualizer`.  \n",
    "This will be done automatically. \n",
    "Your task is to describe and interpret the results in the report:\n",
    "- What differences do you observe between the NN and PINN predictions?\n",
    "- How does the number of training points affect the predictions?\n",
    "- How does the PINN perform compared to the NN?\n",
    "- Is the comparison fair?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5db013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tuples with dataset keys and boolean values for activating physics during training\n",
    "data_list = [\n",
    "    (\"data_0\", False),  \n",
    "    (\"data_0\", True), \n",
    "    (\"data_1\", False), \n",
    "    (\"data_1\", True), \n",
    "    (\"data_2\", False), \n",
    "    (\"data_2\", True)\n",
    "]\n",
    "config_dict = {\n",
    "    \"data_0\": (20, len(time)-1), # Containing (num_train, max_idx)\n",
    "    \"data_1\": (10, len(time)//2), \n",
    "    \"data_2\": (2, len(time)-1)\n",
    "}\n",
    "\n",
    "# Initialize a list to store predictions that will be the result of the training\n",
    "pred_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394abb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the training for the given configurations\n",
    "\n",
    "for data, physics in data_list:\n",
    "    # Print the name of the current dataset\n",
    "    print(f\"Training on dataset {data} with physics={physics}\")\n",
    "    print(f\"Using {config_dict[data][0]} training points and a maximum index of {config_dict[data][1]}\")\n",
    "\n",
    "    # Prepare the data for the current dataset\n",
    "    train_ds, test_ds = ...\n",
    "\n",
    "    # Initialize the model by calling the PINN() class created earlier.\n",
    "    model = ...\n",
    "\n",
    "    # Execute the training for the given data_set\n",
    "    predictions_dict = ...\n",
    "\n",
    "    # Append the predictions to the pred_list\n",
    "    ...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "\n",
    "for i, sub_dict in enumerate(pred_list):\n",
    "    Visualizer = SolutionVisualizer(sub_dict)\n",
    "    Visualizer.plot_solution(title=rf'Physics activated = {data_list[i][1]}',\n",
    "                             filename=f\"NN_PINN_data_{i}.svg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd7750a",
   "metadata": {},
   "source": [
    "## Task 4: Collocation points\n",
    "The goal is to determine how collocation points affect the training of a PINN.  \n",
    "To achieve this, we train the PINN using the follwing configuration and adjust the number of collocation points:\n",
    "- 2 training points using the first and last point of the data set.\n",
    "- Learning rate $\\eta = 0.005$\n",
    "- Training epochs $n_\\text{epochs} = 500$\n",
    "\n",
    "We will test the following number of collocation points:\n",
    "- 2 linearly spaced over time\n",
    "- 5 linearly spaced over time\n",
    "- 20 linearly spaced over time\n",
    "\n",
    "1. Implement the training for each number of collocation points using the following steps.\n",
    "You can organize the code similar to the previous task.\n",
    "\n",
    "2. Plot and compare the loss evolution and final prediction of all four configurations. \n",
    "\n",
    "3. What can you see, and how can you explain the behavior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b5c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store data\n",
    "data_dict = {}\n",
    "pred_list = []\n",
    "\n",
    "num_collocation_points = [2, 5, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop and execute the training for different collocation points\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b26906",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sub_dict in enumerate(pred_list):\n",
    "    Visualizer = SolutionVisualizer(sub_dict)\n",
    "    Visualizer.plot_solution(title=rf'Number of colocation points = {num_collocation_points[i][0]}',\n",
    "                             filename=f\"col_data_{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3597321",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
